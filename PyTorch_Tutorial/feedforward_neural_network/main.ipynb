{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Loads the **CIFAR-100** image dataset(50k train, 10k test; images are 3 * 32 * 32).\n",
    "- Builds a small a neural network (an MLP with one hidden layer).\n",
    "- Trains it to predict one of **100 classes**.\n",
    "- **Evaluates** accuracy on the test set.\n",
    "- **Saves** the trained weights to `model.ckpt`.\n",
    "\n",
    "```\n",
    "images -> numbers (tensors) -> model computes scores -> compare to correct label -> adjust model -> repeat -> test.\n",
    "```"
   ],
   "id": "186443b6792bac3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T13:00:17.516720Z",
     "start_time": "2025-10-25T13:00:11.337471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from sympy import false\n"
   ],
   "id": "5982d5707ed818c0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `torch`: core PyTorch (tensors, GPU, autograd).\n",
    "- `torch.nn as nn`: neural-network layers & losses.\n",
    "- `torchvision`: ready-made datasets/models for vision.\n",
    "- `transformers`: image preprocessing (convert to tensor, normalize, etc).\n",
    "- `from sympy import false`: not needed here - safe to delete."
   ],
   "id": "4032b79d90ce26f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Picking a compute device (CPU/NVIDIA GPU/ APPLE GPU)",
   "id": "6776a6e06486c678"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return  torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")"
   ],
   "id": "77d5b03aabd381a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- On Apple silicon, MPS runs on the built-in GPU.\n",
    "- on PCs with NVIDIA, CUDA runs on the GPU.\n",
    "- otherwise CPU.\n",
    "- we must move both the model and the data to same device."
   ],
   "id": "6b04b605e2eaac6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3) Defining the model (a tiny MLP)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)   # weights + bias [3072 -> 500]\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # [500 -> 100]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)   # flatten image [B, 3, 32, 32] -> [B, 3072]\n",
    "        x = self.fc1(x)     # linear transform\n",
    "        x = self.relu(x)    # ReLU activation\n",
    "        x = self.fc2(x)     # final logits (raw scores) for 100 classes.\n",
    "        return x\n",
    "\n"
   ],
   "id": "a46933106b133a74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Why flatten?** The MLP expects a 1-D vector per image (3072 numbers = 3 * 32 * 32).\n",
    "- **Why ReLU?** Adds non-linearity so the model can learn complex patterns.\n",
    "- **Why return logits (no softmax)?** The loss function you use (CrossEntropy) expects **raw scores** and handles softmax internally (more stable numerically)."
   ],
   "id": "f33a2ee15211f897"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4) main() -> set hyperparameters & data pipeline\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyper-parameters for CIFAR-100\n",
    "input_size = 3 * 32 * 32    # 3072 number per image\n",
    "hidden_size = 500\n",
    "num_classes = 100\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n"
   ],
   "id": "4699bd5cd4d0ab7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- **Hyperparameters** are knobs you choose (not learned): learning rate, batch size, etc.",
   "id": "b854a41bb70ca5fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5) Transforms (preprocessing)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5071, 0.4867, 0.4408),\n",
    "            std=(0.2675, 0.2565, 0.2761)\n",
    "        )\n",
    "    ]\n",
    ")"
   ],
   "id": "e74702e28ef84bcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `transforms.Compose([...])` = build a pipeline of steps. Each image passes through steps in order.\n",
    "- `ToTensor()`:\n",
    "    - Converts a PIL image (HxWxC, 0-255 integers) -> PyTorch tensor (CxHxW, float32) with values in [0,1].\n",
    "    - `Normalize(mean, std)` per channel (R, G, B):\n",
    "        -  For every pixel value `x` (already in [0,1]),\n",
    "                `x_norm = (x - mean[channel]) / std[channel])\n",
    "        - This is standardization: **center**(subtract mean) and **scale** (divide by std).\n",
    "        - Result: each channel's distribution is roughly mean ≈ 0 and std ≈ 1 over the training set.\n",
    "### Why do we normalize?\n",
    "- **Faster, more stable training.** Centered, similarly-scaled features make gradients behave better.\n",
    "- **Help optimization**(Adam/SGD) and can improve final accuracy.\n",
    "- It's a long-standing best practice for image models.\n",
    "\n",
    "### Why these numbers (mean = (0.5071, 0.4867, 0.4408) and std= (0.2675, 0.2565, 0.2761)?\n",
    "    - These are empirical channel means & stds computed on CIFAR-100 training set after `ToTensor()` (so they're in the 0-1 scale).\n",
    "### tl;dr\n",
    "- `Compose` builds a preprocessing pipeline.\n",
    "- `ToTensor()` -> tensor in [0,1].\n",
    "- `Normalize(mean, std)` -> per-channel `(x-mean)/std` using CIFAR-100's own mean/std, which speeds up and stabilized learning."
   ],
   "id": "5a38ed90210f59fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
